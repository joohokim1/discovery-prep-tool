#!/usr/bin/env python
# encoding:utf8

import argparse
import requests
import base64
import subprocess
import os
import copy
import json

parser = argparse.ArgumentParser()
parser.add_argument('-v', '--verbose', help='print out more responses in detail', action='store_true')
parser.add_argument('-H', '--hostname', help='API server hostname (default: localhost)', default='localhost')
parser.add_argument('-p', '--port', help='API server port (default: 8180)', default='8180')
parser.add_argument('-s', '--src-path', help='Source pathname')
parser.add_argument('-d', '--dest-path', help="Destination pathname (When this is a directory, append src's basename")
parser.add_argument('-w', '--wds-id', help='Wrangled dataset ID to apply')
parser.add_argument('-c', '--connection-id', help='Data connection ID')
parser.add_argument('-D', '--datasource-id', help='Datasource ID to ingest into')
parser.add_argument('-i', '--ids-name', help='New imported dataset name', default='anonymous')
parser.add_argument('-q', '--query-stmt', help='Query statement')
parser.add_argument('--search-wds', help='Search wrangled dataset IDs by name')
parser.add_argument('--search-connection', help='Search data connection IDs by name')
parser.add_argument('--search-datasource', help='Search datasource IDs by name')
parser.add_argument('--manual-colcnt', help='Manual max column count when the max column count', default='0')
args = parser.parse_args()


def get_token():
    auth_key = base64.b64encode('polaris_client:polaris'.encode()).decode()
    auth_hdr = {'Content-Type': 'application/json', 'Authorization': 'Basic ' + auth_key}
    r = requests.post(METATRON_URL + '/oauth/token?grant_type=password&username=admin&password=admin', headers=auth_hdr)
    return r.json()['access_token']


def get_upload_policy():
    j = requests.get(DATASETS_URL + '/file_upload', headers=HDR).json()
    return j['upload_id'], j['limit_size']


def post_upload_file(upload_id, limit_size, total_size, basename):
    hdr = copy.deepcopy(HDR)
    del hdr['Content-Type']  # Content-Type becomes multipart/form-data when files is not None
    params = 'name=%s&chunk=0&chunks=1&storage_type=LOCAL&upload_id=%s&chunk_size=%s&total_size=%s' % \
             (basename, upload_id, limit_size, total_size)
    files = {'file': open(args.src_path, 'rb')}
    r = requests.post(DATASETS_URL + '/file_upload?' + params, headers=hdr, files=files)
    return r.json()['storedUri']


def get_file_format(basename):
    if basename.endswith('.xlsx') or basename.endswith('.xls'):
        return 'EXCEL'
    elif basename.endswith('.json'):
        return 'JSON'
    else:
        return 'CSV'


def post_create_file_dataset(basename, stored_uri):
    ids_name = basename if args.ids_name is None else args.ids_name
    data = {'delimiter': ',', 'quoteChar': '"', 'dsName': ids_name, 'dsDesc': '', 'dsType': 'IMPORTED',
            'importType': 'UPLOAD', 'filenameBeforeUpload': basename, 'storageType': 'LOCAL', 'sheetName': '',
            'storedUri': stored_uri, 'fileFormat': get_file_format(basename)}
    if args.manual_colcnt is not None:
        data['manualColcnt'] = args.manual_colcnt
    r = requests.post(DATASETS_URL, headers=HDR, data=json.dumps(data))
    print('File type dataset created: src_path=%s ids_name=%s stored_uri=%s' % (args.src_path, ids_name, stored_uri))


def post_create_database_dataset(dc_id, ids_name, query_stmt):
    assert ids_name is not None and query_stmt is not None
    data = {'dcId': dc_id, 'dsName': ids_name, 'dsType': 'IMPORTED', 'importType': 'DATABASE', 'queryStmt': query_stmt,
            'rsType': 'QUERY'}
    r = requests.post(DATASETS_URL, headers=HDR, data=json.dumps(data))
    if (r.status_code == 200):
        print('Creating database type dataset succeeded: ids_name=%s query_stmt=%s' % (ids_name, query_stmt))
    else:
        print('Creating database type dataset failed: ' + r)


### Start
METATRON_URL = 'http://%s:%s' % (args.hostname, args.port)
DATASETS_URL = METATRON_URL + '/api/preparationdatasets'

HDR = {'Authorization': "bearer " + get_token(), 'Content-Type': 'application/json', 'Accept': 'application/json'}
stored_uri = None

if args.src_path is not None:
    upload_id, limit_size = get_upload_policy()
    total_size = subprocess.check_output("wc -c %s | awk '{print $1'}" % args.src_path, shell=True).decode().strip()
    basename = os.path.basename(args.src_path)
    stored_uri = post_upload_file(upload_id, limit_size, total_size, basename)
    post_create_file_dataset(basename, stored_uri)
else:
    post_create_database_dataset('6110369b-2645-4353-a283-691728de9b84', args.ids_name, args.query_stmt)

# eof
