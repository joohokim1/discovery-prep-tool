#!/usr/bin/env python
# encoding:utf8

import argparse, requests, base64, subprocess, os, copy, json, pprint

parser = argparse.ArgumentParser()
parser.add_argument('-v', '--verbose', help='print out more responses in detail', action='store_true')
parser.add_argument('-H', '--hostname', help='API server hostname (default: localhost)', default='localhost')
parser.add_argument('-p', '--port', help='API server port (default: 8180)', default='8180')
parser.add_argument('-s', '--src-path', help='Source pathname')
parser.add_argument('-d', '--dest-path', help="Destination pathname (When this is a directory, append src's basename")
parser.add_argument('-w', '--wds-id', help='Wrangled dataset ID to apply')
parser.add_argument('-c', '--connection-id', help='Data connection ID')
parser.add_argument('-D', '--datasource-id', help='Datasource ID to ingest into')
parser.add_argument('-i', '--ids-name', help='New imported dataset name', default='anonymous')
parser.add_argument('-q', '--query-stmt', help='Query statement')
parser.add_argument('-E', '--export-df-id', help='Export a dataflow as JSON')
parser.add_argument('-I', '--import-df-file', help='Import a dataflow from JSON file')
parser.add_argument('--search-wds', help='Search wrangled dataset ID by name')
parser.add_argument('--search-connection', help='Search data connection ID by name')
parser.add_argument('--search-datasource', help='Search datasource ID by name')
parser.add_argument('--search-dataflow', help='Search dataflow ID by name')
parser.add_argument('--manual-colcnt', help='Manual max column count when the max column count', default='0')
args = parser.parse_args()


# 1. Get auth token
def get_token():
    auth_key = base64.b64encode('polaris_client:polaris'.encode()).decode()
    auth_hdr = {'Content-Type': 'application/json', 'Authorization': 'Basic ' + auth_key}
    r = requests.post(METATRON_URL + '/oauth/token?grant_type=password&username=admin&password=admin', headers=auth_hdr)
    return r.json()['access_token']


# 2. Get upload policy
def get_upload_policy():
    j = requests.get(DATASETS_URL + '/file_upload', headers=HDR).json()
    return j['upload_id'], j['limit_size']


# 3. Upload file chunks
def post_upload_file(upload_id, limit_size, total_size, basename):
    hdr = copy.deepcopy(HDR)
    del hdr['Content-Type']  # Content-Type becomes multipart/form-data when files is not None
    params = 'name=%s&chunk=0&chunks=1&storage_type=LOCAL&upload_id=%s&chunk_size=%s&total_size=%s' % \
             (basename, upload_id, limit_size, total_size)
    files = {'file': open(args.src_path, 'rb')}
    r = requests.post(DATASETS_URL + '/file_upload?' + params, headers=hdr, files=files)
    return r.json()['storedUri']


# 4. Search data connection
def search_data_connection(dc_name):
    r = requests.post(CONNECTIONS_URL + '/filter?projection=list&page=0&size=20&sort=createdTime,desc', headers=HDR,
                      data=json.dumps({'containsText': dc_name}))
    for d in r.json()['_embedded']['connections']:
        return d['id']
    return None


def get_file_format(basename):
    if basename.endswith('.xlsx') or basename.endswith('.xls'):
        return 'EXCEL'
    elif basename.endswith('.json'):
        return 'JSON'
    else:
        return 'CSV'


# 5-1. Create imported dataset - file
def post_create_file_dataset(basename, stored_uri):
    ids_name = basename if args.ids_name is None else args.ids_name
    data = {'delimiter': ',', 'quoteChar': '"', 'dsName': ids_name, 'dsType': 'IMPORTED',
            'importType': 'UPLOAD', 'filenameBeforeUpload': basename, 'storageType': 'LOCAL',
            'storedUri': stored_uri, 'fileFormat': get_file_format(basename)}
    if args.manual_colcnt is not None:
        data['manualColcnt'] = args.manual_colcnt
    r = requests.post(DATASETS_URL, headers=HDR, data=json.dumps(data))
    if r.status_code == 200:
        print('File dataset created: src_path=%s ids_name=%s stored_uri=%s' % (args.src_path, ids_name, stored_uri))
    else:
        print('Creating file type dataset failed: ' + str(r))


# 5-2. Create imported dataset - query
def post_create_database_dataset(dc_id, ids_name, query_stmt, db_name=None, tbl_name=None):
    assert ids_name is not None
    if query_stmt is not None:
        data = {'dcId': dc_id, 'dsName': ids_name, 'dsType': 'IMPORTED', 'importType': 'DATABASE',
                'queryStmt': query_stmt, 'rsType': 'QUERY'}
    else:
        data = {'dcId': dc_id, 'dsName': ids_name, 'dsType': 'IMPORTED', 'importType': 'DATABASE',
                'dbName': db_name, 'tblName': tbl_name, 'rsType': 'TABLE'}

    r = requests.post(DATASETS_URL, headers=HDR, data=json.dumps(data))
    if r.status_code == 200:
        print('Database dataset created: ids_name=%s query_stmt=%s' % (ids_name, query_stmt))
    else:
        print('Creating database type dataset failed: ' + str(r))


# 6. Search wrangled dataset
def search_wrangled_dataset():
    r = requests.get(DATASETS_URL + '/search/findByDsNameContaining?dsName=' + args.search_wds, headers=HDR)
    for d in r.json()['_embedded']['preparationdatasets']:
        args.wds_id = args.wds_id if args.wds_id else d['dsId']


# 7. Get dataset details
def get_dataset_details(ds_id):
    d = requests.get(DATASETS_URL + '/' + ds_id, headers=HDR).json()
    print(json.dumps(d, indent=4))


def trim(d, keep_keys):
    return dict((key, d[key]) for key in keep_keys if key in d)


def export_dataset(ds_id):
    d = requests.get(DATASETS_URL + '/' + ds_id, headers=HDR).json()
    return trim(d, ['dsName', 'dsDesc', 'dsType', 'importType', 'rsType', 'dbName', 'tblName', 'queryStmt',
                    'transformRules', 'storedUri', 'dcName'])


def search_dataflow():
    r = requests.get(DATAFLOWS_URL + '/search/findByDfNameContaining?dfName=' + args.search_dataflow, headers=HDR)
    for d in r.json()['_embedded']['preparationdataflows']:
        args.export_df_id = args.export_df_id if args.export_df_id else d['dfId']


def export_dataflow(df_id):
    d = requests.get(DATAFLOWS_URL + '/' + df_id, headers=HDR).json()
    rd = {'datasets': [], 'dataflow': trim(d, ['dfName', 'dfDesc'])}
    for dataset in d['datasets']:
        rd['datasets'].append(export_dataset(dataset['dsId']))
    print(json.dumps(rd, indent=4))


def import_ids(dsl):
    for d in dsl:
        if d['dsType'] == 'IMPORTED':
            # NOTE: UPLOAD type I.DS should be created in advance.
            if d['importType'] == 'URI':
                post_create_file_dataset(os.path.basename(d['storedUri']), d['storedUri'])
            elif d['importType'] == 'DATABASE':
                dc_id = search_data_connection(d['dcName'])
                if dc_id is None:
                    continue
                if 'queryStmt' in d:
                    post_create_database_dataset(dc_id, d['dsName'], d['queryStmt'])
                else:
                    post_create_database_dataset(dc_id, d['dsName'], None, d['dbName'], d['tblName'])


def import_dataflow(d):
    pass


def import_wds(dsl):
    pass


# === Start ===
METATRON_URL = 'http://%s:%s' % (args.hostname, args.port)
DATASETS_URL = METATRON_URL + '/api/preparationdatasets'
DATAFLOWS_URL = METATRON_URL + '/api/preparationdataflows'
CONNECTIONS_URL = METATRON_URL + '/api/connections'

HDR = {'Authorization': "bearer " + get_token(), 'Content-Type': 'application/json', 'Accept': 'application/json'}

if args.connection_id is None and args.search_connection:
    args.connection_id = search_data_connection(args.search_connection)

if args.src_path:
    upload_id, limit_size = get_upload_policy()
    total_size = subprocess.check_output("wc -c %s | awk '{print $1'}" % args.src_path, shell=True).decode().strip()
    basename = os.path.basename(args.src_path)
    stored_uri = post_upload_file(upload_id, limit_size, total_size, basename)
    post_create_file_dataset(basename, stored_uri)
elif args.query_stmt:
    post_create_database_dataset(args.connection_id, args.ids_name, args.query_stmt)

if args.search_wds:
    search_wrangled_dataset()

if args.search_dataflow:
    search_dataflow()

if args.export_df_id:
    export_dataflow(args.export_df_id)

if args.import_df_file:
    with open(args.import_df_file, 'rb') as f:
        d = json.loads(f.read().decode())
        import_ids(d['datasets'])
        import_dataflow(d['dataflow'])
        import_wds(d['datasets'])

# eof
